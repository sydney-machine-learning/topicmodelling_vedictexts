# -*- coding: utf-8 -*-
"""upnishadas_cleaning_topic_modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CjQZ9zjZVHZeHDaHkJBc_jq8jtoQZrnQ
"""

# !pip install top2vec
# !pip install top2vec[sentence_encoders]
# !pip install top2vec[sentence_transformers]
# !pip install plotly==5.2.1
# !pip install -U kaleido

#Basic Python and Machine learning libraries
import os, sys, warnings, random, time, re, math, string, copy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

import json
from string import punctuation
from collections import Counter,defaultdict
from re import search
from scipy import stats

from wordcloud import WordCloud, STOPWORDS

from sklearn import metrics
from sklearn.model_selection import StratifiedKFold

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel

# sklearn data science models
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso
from sklearn.svm import LinearSVC
import xgboost as xgb

from bs4 import BeautifulSoup

import nltk

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from nltk.tokenize.casual import casual_tokenize
from nltk.util import ngrams
from nltk import pos_tag

import spacy
warnings.filterwarnings('ignore')

#tqdm with pandas
from tqdm import tqdm
tqdm.pandas()
from collections import Counter

from natsort import natsorted
from preprocess_utils import contraction_mapping, map_old_new

def set_seed(seed = 42):
    '''Sets the seed of the entire notebook so results are the same every time we run.
    This is for REPRODUCIBILITY.'''
    np.random.seed(seed)
    random.seed(seed)
    # torch.manual_seed(seed)
    # torch.cuda.manual_seed(seed)
    # # When running on the CuDNN backend, two further options must be set
    # torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = True
    # Set a fixed value for the hash seed
    os.environ['PYTHONHASHSEED'] = str(seed)
set_seed()

fname = '/content/drive/MyDrive/intern-unsw/assets/upnishads_texts/108upanishads.txt'
with open(fname, 'r', encoding='utf-8-sig') as f:
  data = f.read()
  f.close()

def veda_to_upanishads(raw_data):
  '''
  get the names of all 108 upanishads from raw data
  Args:
    raw_data: unprocessed data
  Return:
    all_upanishads: dictioanry with vedas name as key and upanishads list as values
  '''
  raw_data = raw_data.replace('\x0c', '')#remove unicode
  raw_data = raw_data.replace('*', '')#remove asterics
  data_split_newline = raw_data.split('\n')
  
  veda_to_upanishads = defaultdict(list)

  
  all_upanishads_rig_ = [x for x in data_split_newline[33880:33891]]
  all_upanishads_syv_ = [x for x in data_split_newline[33893:33912]]
  all_upanishads_kyv_ = [x for x in data_split_newline[33914:33947]]
  all_upanishads_sam_ = [x for x in data_split_newline[33949:33965]]
  all_upanishads_ath_ = [x for x in data_split_newline[33967:33999]]


  #Rig Veda
  for i, name in enumerate(all_upanishads_rig_):
    if len(name) > 0:
      name = name.strip().split(' ')[1] 
      veda_to_upanishads["Rig-Veda"].append(name)
  #Sukla_yajur_veda

  for i, name in enumerate(all_upanishads_syv_):
    if len(name) > 0:
      name = name.strip().split(' ')[1] 
      veda_to_upanishads["Sukla-Yajur-Veda"].append(name)
  # Krishna-Yajur-Veda

  for i, name in enumerate(all_upanishads_kyv_):
    if len(name) > 0:
      name = name.strip().split(' ')[1] 
      veda_to_upanishads["Krishna-Yajur-Veda"].append(name)
  # Sama-Veda

  for i, name in enumerate(all_upanishads_sam_):
    if len(name) > 0:
      name = name.strip().split(' ')[1] 
      veda_to_upanishads["Sama-Veda"].append(name)

  # Atharva-Veda
  for i, name in enumerate(all_upanishads_ath_):
    if len(name) > 0:
      name = name.strip().split(' ')[1] 
      veda_to_upanishads["Atharva-Veda"].append(name)
  return veda_to_upanishads
##Create dictionary for each Upanishads
veda_upanishads = veda_to_upanishads(data)
# veda_upanishads

def get_upanishads_names(raw_data):
  '''
  get the names of all 108 upanishads from raw data
  Args:
    raw_data: unprocessed data
  Return:
    all_upanishads: list of all 108 upanishads
  '''
  raw_data = raw_data.replace('\x0c', '')#remove unicode
  raw_data = raw_data.replace('*', '')#
  data_split_newline = raw_data.split('\n')
  
  all_upanishads = []
  all_upanishads_ = [x for x in data_split_newline[16:128]]

  for i, name in enumerate(all_upanishads_):
    if len(name) > 0 and "Followed by" not in name:
      name = name.strip().split(' ')[1] 
      all_upanishads.append(name)
  return all_upanishads
##Create dictionary for each Upanishads
all_upanishads = get_upanishads_names(data)
# len(all_upanishads)

# re.findall(r"End of .*Commentary", data)
# re.findall(r"(Here ends the .*panishad.*Veda\.)", data)

def create_upanishads_dict(raw_data):
  '''
  Create a dictionary for each Upanished contained with texts 
  '''
  # all_upanishads = get_upanishads_names(raw_data)
  # print(len(all_upanishads))
  raw_data = data.replace('\x0c', '')
  all_match = re.findall(r"(Here ends the .*panishad.*Veda\.)", raw_data)

  #both part contais the explanation of same upanishad so combine them
  #all_match.remove('Here ends the Mandukyopanishad, included in the Atharva-Veda.')
  all_match.remove('Here ends the Mandukyopanishad, as contained in the Atharva-Veda.')
  all_match.remove('Here ends the Nrisimha Poorva Tapaniyopanishad, as contained in the Atharva-Veda.')
  #print(len(all_match))
  #some text contains commentry by Swami Nirmalananda Giri
  #print(len(all_upanishads),len(all_match))
  commentary_list = re.findall(r"End of .*Commentary", raw_data)

  upan_dict = {}
  start_idx = raw_data.find("Isavasya Upanishad");

  for idx , match in enumerate(all_match):
    upan = match.strip().split(" ")[3]
    #upan = all_upanishads[idx]
    # print(upan, ":", match)
    if upan.endswith(","):
      upan = upan[:-1]#remove comma in the end
    #print(upan)
    end_idx = raw_data.find(all_match[idx])

    #first 10 upanishads contains commentary
    if idx < 10:
      end_idx = raw_data.find(commentary_list[idx])
      upan_dict[upan] = r"{}".format(raw_data[start_idx:end_idx+len(commentary_list[idx])])
      start_idx = end_idx+len(commentary_list[idx])+1
    else:
      upan_dict[upan] = r"{}".format(raw_data[start_idx:end_idx+len(match)])
      start_idx = end_idx+len(match)+1
    # if upan == 'Mandukyopanishad':
    #   print(upan_dict[upan])
  return upan_dict
upan_dict = create_upanishads_dict(data)

def rename_keys_upanishad_dict(new_key_list, upanishad_dict):
  '''
  rename keys to simple names of upanishads
  return:
      upanishad dict with renamed keys
  '''
  upanishad_dict = dict(zip(new_key_list,list(upanishad_dict.values())))

  return upanishad_dict

new_upan_dict = rename_keys_upanishad_dict(all_upanishads, upan_dict)

with open('all_upanishads.json', 'w') as fp:
    json.dump(new_upan_dict, fp,indent=4)

def remove_beginning_lines(data):
  """
  Remove the beginning lines, like translated by, published by, and upanishads name
  """
  data_list = data.split('\n')
  
  data_list = [x for x in data_list if len(x)> 0]
  
  data_list = data_list[1:]#remove name of the upanishads
  
  if "Translated by" in data_list[0]:
    data_list = data_list[1:]
  if "Published by" in data_list[0]:
    data_list = data_list[1:]

  data = "\n".join(data_list)
  
  return data

def replace_words_contraction(data, map_old_new, contraction_map):
  """
  Replace old and archaic words
  """
  for new_old, word in zip(map_old_new.keys(), contraction_map.keys()):
    data = data.replace(new_old, map_old_new[new_old])
    data = data.replace(word, contraction_map[word])
  return data

# data_removed_begin = remove_beginning_lines(new_upan_dict['Isa'])
# print(data_removed_begin)
text = new_upan_dict['Isa']

def lemmatize_sentence(sentence):
  """
  Lemmatize a single sentence
  """
  lemma_sentence=[]
  wnl = WordNetLemmatizer()
  for word, tag in pos_tag(word_tokenize(sentence)):
    wntag = tag[0].lower()
    wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None
    if not wntag:
      lemma = word
    else:
      lemma = wnl.lemmatize(word, wntag)
    lemma_sentence.append(lemma)
    lemma_sentence.append(" ")
  return "".join(lemma_sentence)

def lemmatize_doc(data, remove_stopwords = False):
  """
  Lemmatize entire documents
  """
  data_list = data.split('\n')
  
  data_list = [x for x in data_list if len(x)> 0]
  lametized_list = []
  for sentence in data_list:
    lam_sent = lemmatize_sentence(sentence)
    if remove_stopwords:
      lam_sent = remove_stop_words(lam_sent)

    lametized_list.append(lam_sent)

  lametized_list = [x for x in lametized_list if len(x)> 0]
  data = "\n".join(lametized_list)
  
  return data

def remove_stop_words(text):
  sw_nltk = stopwords.words('english')
  words = [word for word in text.split() if word.lower() not in sw_nltk]
  new_text = " ".join(words)
  return new_text

def clean_text(data, remove_stopwords = False):
  '''
  clean the raw text of each of the 108 upanishads
  '''
  #remove unicode
  data = data.replace('\x0c', '')

  #removes beginning line eg. Upanishad's name, translated by and published by
  data = remove_beginning_lines(data)

  #remove sloka's numbering eg. 2-III-11.,I-3:,10.etc
  data = re.sub('(.*[-][\d]+[.:])|(^[\d]+[.])', '', data)

  #remove the numbering of upanishads eg. (Atharvashikha Upanishad 1:10)
  data = re.sub('[(](?<=\()(.*? Upanishad .*?)(?=\))[)]', '', data)

  #remove the numbering of upanishads eg. (2:10)
  data = re.sub('[(](?<=\()([0-9]+:[0-9]+)(?=\))[)]', '', data)

  #remove the numberings eg. II-ii-51-56., II-21.
  data = re.sub("(.*[-][\d]+[.:])|(^[\d]+[.])|(.*[-][\d]+([a-z()]{3})[.:])", '', data)

  #remove and replace different symbols and repetitive characters
  data = data.replace('…………………', '')
  data = data.replace('………..', '')
  data = data.replace('……', '')
  data = data.replace('....', '')
  data = data.replace('...', '')
  data = data.replace('..', '')

  #replace texts
  data = data.replace('Om!', 'Om ')
  data = data.replace('Om !', 'Om')
  data = data.replace('Om! Peace! Peace! Peace!', 'Om Peace Peace Peace!')

  #remove extra texts like commentary by ...
  data = re.sub('.*by Swami Nirmalananda Giri.', '', data)
  data = re.sub(r"End of .*Commentary", '', data)
  
  #eg. Isha Upanishad commentary
  data = re.sub(".*panishad Commentary", '', data)

  #remove archaic words like thy thou etc
  data = replace_words_contraction(data, map_old_new, contraction_mapping)

  # remove all other symbols numbers and white spaces
  data = "".join([character if (character.isalpha() or character == "." or character ==" " or character == "\n") else " " for character in data])
  
  # remove line break and break based on full stops to make documents
  data = data.replace("\n", " ")

  data_list = data.split(".")
  data_list = [x for x in data_list if (len(x.strip().split(" ")) > 3)]

  data = "\n".join(data_list)

  #remove extra spaces 
  data = re.sub(" +", " ", data)

  #lower case
  data = data.lower()
  
  #lametization
  data = lemmatize_doc(data, remove_stopwords)
  
  return data

def get_cleaned_data_dict(data):
  """
  dict of Cleaned data
  """
  all_upanishads = get_upanishads_names(data)
  upan_dict = create_upanishads_dict(data)
  new_upan_dict = rename_keys_upanishad_dict(all_upanishads, upan_dict)

  cleaned_upanishads_dict = {}
  for upanishads_name in new_upan_dict.keys():
    cleaned_upanishads_dict[upanishads_name] = clean_text(new_upan_dict[upanishads_name], remove_stopwords=True)
  
  return cleaned_upanishads_dict

def save_cleaned_data_as_json(data):
  cleaned_upanishads = get_cleaned_data_dict(data)
  with open('all_upanishads_cleaned.json', 'w') as fp:
    json.dump(cleaned_upanishads_dict, fp, indent=4)

cleaned_upanishads = get_cleaned_data_dict(data)

def join_upanishads(data, num):
  cleaned_upanishads = get_cleaned_data_dict(data)
  top_num_upanishads = []
  for idx, name in enumerate(cleaned_upanishads.keys()):
    if idx < num:
      top_num_upanishads.append(cleaned_upanishads[name])
  
  top_num_upanishads_ = "\n".join(top_num_upanishads)
  return top_num_upanishads_
#top_twelve_upanishads = join_upanishads(data, 12)
principal_upanishads = join_upanishads(data, 12)

"""# Topic modelling using Top2vec"""

from top2vec import Top2Vec
import umap
import hdbscan

def get_topics_model(documents, embedding_model='distiluse-base-multilingual-cased', num_topics = 10, speed = 'learn'):
  """
  Returns model and topics
  embedding_models: `universal-sentence-encoder`
                    `universal-sentence-encoder_-multilingual`
                    `distiluse-base-multilingual-cased`
  speed:            `learn`
                    `deep-learn`
                    `fast-learn`
  """
  model = Top2Vec(documents = documents, speed = speed, workers = 8, min_count = 2, embedding_model= embedding_model)
  if model.get_num_topics() > num_topics:
    topic_words, word_scores, topic_nums = model.get_topics(num_topics)
  else:
    topic_words, word_scores, topic_nums = model.get_topics()
  return model, topic_words, word_scores

def get_words(model, num_words = 20, num_topics = 10):
  """
  get n random words from each topics
  """
  reduced_flag = False
  if model.get_num_topics() > num_topics:
    reduced_topic_lists = model.hierarchical_topic_reduction(num_topics = num_topics)
    reduced_flag = True
  
  topic_words, word_scores, topic_nums = model.get_topics(reduced=reduced_flag)
  return topic_words[:,:num_words], word_scores[:,:num_words], topic_nums

model = Top2Vec(documents= principal_upanishads.split("\n"), speed='deep-learn', workers=8, min_count = 2, embedding_model='distiluse-base-multilingual-cased')

topic_words, word_scores, topic_nums = model.get_topics()
print(model.get_num_topics())

# save topics
topic_words, word_scores, topic_nums = model.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-principal-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('principal_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 2)

topic_words, word_scores, topic_nums = model.get_topics(14)
topic_words
# for topic in topic_nums:
#     model.generate_topic_wordcloud(topic)

#np.random.seed(333)
def get_embeddings_df_2d(model, num_reduced_topics = 10, sigma = [3,3] ):
    
  """
  2d plot the documents embedding
  """
  umap_args = {
    "n_neighbors": 15,
    "n_components": 2, # 5 -> 2 for plotting 
    "metric": "cosine",
    }
  umap_data = umap.UMAP(**umap_args).fit_transform(model._get_document_vectors(norm=False))

  #get dataframe of the result 
  result = pd.DataFrame(umap_data, columns=['x_embeddings', 'y_embeddings'])
  result['Labels'] = np.array( ['Topic-'+ str(x+1) if x != -1 else 'outliers' for x in list(model.doc_top) ] )

  #### Hierarchical Topic Reduction ##########################
  new_label = []
  if model.get_num_topics() > num_reduced_topics:
    reduced_topic_lists = model.hierarchical_topic_reduction(num_topics = num_reduced_topics)
    ######################################

    ###get new labels
    for top_doc in model.doc_top:
      for idx, reduced_list in enumerate(reduced_topic_lists):
        if top_doc in reduced_list:
          new_label.append("Topic-"+str(idx+1))
          break
    ##################################
    result["Labels"] = np.array(new_label)
  else:
    result["Labels"] = result['Labels']

  #remove outliers
  result = result[np.abs(result.x_embeddings-result.x_embeddings.mean()) <= (sigma[0]*result.x_embeddings.std())]
  result = result[np.abs(result.y_embeddings-result.y_embeddings.mean()) <= (sigma[1]*result.y_embeddings.std())]
  return result


def get_embeddings_df_3d(model, num_reduced_topics = 10, sigma = [3,3,3]):

  """
  3d plot the documents embedding
  """
  umap_args = {
    "n_neighbors": 15,
    "n_components": 3, # 5 -> 2 for plotting 
    "metric": "cosine",
    }
  umap_data = umap.UMAP(**umap_args).fit_transform(model._get_document_vectors(norm=False))
  result = pd.DataFrame(umap_data, columns=['x_embeddings', 'y_embeddings', 'z_embeddings'])
  result['Labels'] = model.doc_top

  #get dataframe of the result 
  result['Labels'] = np.array( ['Topic-'+ str(x+1) if x != -1 else 'outliers' for x in list(model.doc_top) ] )
  
  #### Hierarchical Topic Reduction ##########################
  new_label = []
  if model.get_num_topics() > num_reduced_topics:
    reduced_topic_lists = model.hierarchical_topic_reduction(num_topics = num_reduced_topics)
    ######################################

    ###get new labels
    for top_doc in model.doc_top:
      for idx, reduced_list in enumerate(reduced_topic_lists):
        if top_doc in reduced_list:
          new_label.append("Topic-"+str(idx+1))
          break
    ##################################
    result["Labels"] = np.array(new_label)
  else:
    result["Labels"] = result['Labels']

  #remove outliers
  result = result[np.abs(result.x_embeddings-result.x_embeddings.mean()) <= (sigma[0]*result.x_embeddings.std())]
  result = result[np.abs(result.y_embeddings-result.y_embeddings.mean()) <= (sigma[1]*result.y_embeddings.std())]
  result = result[np.abs(result.z_embeddings-result.z_embeddings.mean()) <= (sigma[2]*result.z_embeddings.std())]
  return result

result2d = get_embeddings_df_2d(model, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d = get_embeddings_df_3d(model, num_reduced_topics = 10, sigma = [2.5,2.5,2.5])

sns.set(font_scale=3)
# font = {'size'   : 30}
# plt.rc('font', **font)
def plot2d_df(result, palet, img_name = 'no_name_2d_plot.pdf'):
  clrs = sns.color_palette(palet).as_hex()
  # color_palette = [cpt for cpt in clrs]
  #palette = sns.color_palette(palet)
  x = result['x_embeddings']
  y = result['y_embeddings']
  # z = result['z_embeddings']
  fig = plt.figure(figsize=(24,16))
  ax = fig.add_subplot(111)
  
  ax.set_xlabel('UMAP Embedding (dim = 1)')
  ax.set_ylabel('UMAP Embedding (dim = 2)')

  fig.patch.set_facecolor('lavender')
  ax.set_facecolor('lavender')

  # result = result.rename(columns={'x_embeddings':'UMAP Embedding (dim = 1)', 'y_embeddings':'UMAP Embedding (dim = 2)'})
  result = result.sort_values(by=['Labels'])
  # facet = sns.lmplot(data=result, x='UMAP Embedding (dim = 1)', y='UMAP Embedding (dim = 2)', hue='Labels', 
  #                  fit_reg=False, legend=False, height=10, aspect=1.5, palette = color_palette)

  for idx, topics in enumerate(result.Labels.unique()):
      ax.scatter(x[result.Labels == topics],y[result.Labels == topics],label = topics, c = clrs[idx])
  
  ax = plt.gca()

  #set y-axes to right side
  ax.yaxis.set_label_position("right")
  ax.yaxis.tick_right()

  handles, labels = ax.get_legend_handles_labels()
  # sort both labels and handles by labels
  labels, handles = zip(*natsorted(zip(labels, handles), key=lambda t: t[0]))
  ax.legend(handles, labels, loc='center right', bbox_to_anchor=(0.15, 0.75), title="Labels")
  plt.tight_layout()
  plt.savefig(img_name, bbox_inches = 'tight',dpi = 200, facecolor=fig.get_facecolor(), edgecolor='none')

def plot3d_df(result, palet, img_name = 'no_name_image_3d.pdf'):
  
  result = result.sort_values(by=['Labels'])
  fig = plt.figure(figsize=(24,16))
  ax = fig.add_subplot(111, projection='3d')

  fig.patch.set_facecolor('lavender')
  ax.set_facecolor('lavender')

  x = result['x_embeddings']
  y = result['y_embeddings']
  z = result['z_embeddings']
  
  ax.set_xlabel('UMAP Embedding (dim = 1)', labelpad = 40.0)
  ax.set_ylabel('UMAP Embedding (dim = 2)', labelpad = 40.0)
  ax.set_zlabel('UMAP Embedding (dim = 3)', labelpad = 40.0)

  clrs = sns.color_palette(palet).as_hex()

  for idx, topics in enumerate(result.Labels.unique()):
      ax.scatter(x[result.Labels == topics],y[result.Labels == topics],
                 z[result.Labels == topics],label = topics, c = clrs[idx])
  
  handles, labels = ax.get_legend_handles_labels()
  # sort both labels and handles by labels
  labels, handles = zip(*natsorted(zip(labels, handles), key=lambda t: t[0]))
  ax.legend(handles, labels, loc='best', bbox_to_anchor=(0.15, 1.0), title="Labels")
  plt.tight_layout()
  plt.savefig(img_name, dpi = 150)

# result2d = get_embeddings_df_2d(model, num_reduced_topics = 10, sigma = [2.5,2.5])
# result3d = get_embeddings_df_3d(model, num_reduced_topics = 10, sigma = [2.5,2.5,2.5])

# plt.rcParams["figure.figsize"] = (18,12)

plot2d_df(result2d, 'bright', '12_principal_upanishads_2d.pdf')

plot3d_df(result3d, 'bright', "12_principal_upanishads_3d.pdf")

words_principal_upanishads, _, _ = get_words(model)
words_principal_upanishads

"""## Analyse Upanishads According to Vedas"""

correction = {'Atharva-Veda':{'Mundaka':'Munda', 'Atahrvasiras':'Atahrvasirah',
                      'Ramatapini': 'Ramatapani', 'Pasupatabrahmana':'Pasupata',
                      'Bhasmajabala':'Bhasma'},
              'Krishna-Yajur-Veda':{'Kathavalli':'Katha', 'Taittiriyaka':'Taittiri',
                       'Svetasvatara': 'Svetasva', 'Avadhuta':'Avadhutaka',
                       'Yoga-kundalini': 'Yoga-kundali'},
              'Sama-Veda': {'Maitrayani':'Maitrayini','Rudrakshajabala':'Rudraksha'},
              'Sukla-Yajur-Veda': {'Isavasya':'Isa', 'Trisikhibrahmana':'Trisikhi'}
              }
def get_upan_for_vedas(data, vedas_name = 'Atharva-Veda'):
  map_veda_to_upanishads = veda_to_upanishads(data)
  upan_list = map_veda_to_upanishads[vedas_name]
  cleaned_upanishads = get_cleaned_data_dict(data)
  veda_upanishads = []
  for idx, name in enumerate(upan_list):
    if vedas_name in correction.keys():
      if name in correction[vedas_name].keys():
        name = correction[vedas_name][name]
    veda_upanishads.append(cleaned_upanishads[name])
  veda_upanishads_ = "\n".join(veda_upanishads)
  return veda_upanishads_

atharva_upanishads = get_upan_for_vedas(data, vedas_name = 'Atharva-Veda')
rig_upanishads = get_upan_for_vedas(data, vedas_name = 'Rig-Veda')
sama_upanishads = get_upan_for_vedas(data, vedas_name = 'Sama-Veda')
krishana_upanishads = get_upan_for_vedas(data, vedas_name = 'Krishna-Yajur-Veda')
sukla_upanishads = get_upan_for_vedas(data, vedas_name = 'Sukla-Yajur-Veda')

print(len(rig_upanishads.split('\n')),len(sama_upanishads.split('\n') ),len(atharva_upanishads.split('\n')) )
print(len(krishana_upanishads.split('\n')),len(sukla_upanishads.split('\n') ) )

"""## Atharva Veda"""

# model_atharva, _,_ = get_topics_model(atharva_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_atharva, _,_ = get_topics_model(atharva_upanishads.split('\n'), embedding_model='distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_atharva = get_embeddings_df_2d(model_atharva, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_atharva = get_embeddings_df_3d(model_atharva, num_reduced_topics = 10, sigma = [2,2.5,2.5])

# save topics
topic_words, word_scores, topic_nums = model_atharva.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-atharva-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('atharva_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

plot2d_df(result2d_atharva, 'bright', img_name = 'upanishads_atharva_veda_2d.pdf')
plot3d_df(result3d_atharva, 'bright', img_name = 'upanishads_atharva_veda_3d.pdf')

get_words(model_atharva)

"""## Rig Veda"""

# model_rig, _,_ = get_topics_model(rig_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_rig, _,_ = get_topics_model(rig_upanishads.split('\n'), embedding_model='distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_rig = get_embeddings_df_2d(model_rig, num_reduced_topics = 10, sigma = [2,2])
result3d_rig = get_embeddings_df_3d(model_rig, num_reduced_topics = 10, sigma = [1.5, 2.5,2.5])

result2d_rig = get_embeddings_df_2d(model_rig, num_reduced_topics = 10, sigma = [1.5,1.3])

# plot2d_df(result2d_rig, 'bright', img_name = 'upanishads_rig_veda_2d_1.png')

plot2d_df(result2d_rig, 'bright', img_name = 'upanishads_rig_veda_2d.pdf')
get_words(model_rig)

result3d_rig = get_embeddings_df_3d(model_rig, num_reduced_topics = 10, sigma = [1.8, 1.8,2])
plot3d_df(result3d_rig, 'bright', img_name = 'upanishads_rig_veda_3d.pdf')

# save topics
topic_words, word_scores, topic_nums = model_rig.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-rig-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('rig_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

"""## Sama Veda"""

# model_sama, _,_ = get_topics_model(sama_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_sama, _,_ = get_topics_model(sama_upanishads.split('\n'), embedding_model = 'distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_sama = get_embeddings_df_2d(model_sama, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_sama = get_embeddings_df_3d(model_sama, num_reduced_topics = 10, sigma = [2.5, 2.5,2.5])

# save topics
topic_words, word_scores, topic_nums = model_sama.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-sama-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('sama_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

plot2d_df(result2d_sama, 'bright', img_name = 'upanishads_sama_veda_2d.pdf')
plot3d_df(result3d_sama, 'bright', img_name = 'upanishads_sama_veda_3d.pdf')
get_words(model_sama)

"""## Krishana Veda"""

# model_krishana, _,_ = get_topics_model(krishana_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_krishana, _,_ = get_topics_model(krishana_upanishads.split('\n'), embedding_model='distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_krishana = get_embeddings_df_2d(model_krishana, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_krishana = get_embeddings_df_3d(model_krishana, num_reduced_topics = 10, sigma = [2.5, 2.5,2.5])
plot2d_df(result2d_krishana, 'bright', img_name = 'upanishads_krishana_veda_2d.pdf')
plot3d_df(result3d_krishana, 'bright', img_name = 'upanishads_krishana_veda_3d.pdf')
get_words(model_krishana)

# save topics
topic_words, word_scores, topic_nums = model_krishana.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-krishna-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('krishana_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

"""## Sukla Veda"""

# model_sukla, _,_ = get_topics_model(sukla_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_sukla, _,_ = get_topics_model(sukla_upanishads.split('\n'), embedding_model = 'distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_sukla = get_embeddings_df_2d(model_sukla, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_sukla = get_embeddings_df_3d(model_sukla, num_reduced_topics = 10, sigma = [2.5, 2.5,2.5])
plot2d_df(result2d_sukla, 'bright', img_name = 'upanishads_sukla_veda_2d.pdf')
plot3d_df(result3d_sukla, 'bright', img_name = 'upanishads_sukla_veda_3d.pdf')
print(get_words(model_sukla))

# save topics
topic_words, word_scores, topic_nums = model_sukla.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-sukla-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('sukla_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

"""## Combine Sukla and Krishana Yajur Veda to form **Yajur Veda**"""

yajur_upanishads = '\n'.join([sukla_upanishads, krishana_upanishads])
len(yajur_upanishads.split('\n'))

# model_yajur, _,_ = get_topics_model(yajur_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 10)
model_yajur, _,_ = get_topics_model(yajur_upanishads.split('\n'), embedding_model = 'distiluse-base-multilingual-cased', num_topics = 10, speed = 'deep-learn')
result2d_yajur = get_embeddings_df_2d(model_yajur, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_yajur = get_embeddings_df_3d(model_yajur, num_reduced_topics = 10, sigma = [2.5, 2.5,2.5])
plot2d_df(result2d_yajur, 'bright', img_name = 'upanishads_yajur_veda_2d.pdf')
plot3d_df(result3d_yajur, 'bright', img_name = 'upanishads_yajur_veda_3d.pdf')
get_words(model_yajur)

# save topics
topic_words, word_scores, topic_nums = model_yajur.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-yajur-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('yajur_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

"""## Complete 108 Upanishads"""

all_108_upanishads = join_upanishads(data, 108)
# model_all, _,_ = get_topics_model(all_108_upanishads.split('\n'), embedding_model='universal-sentence-encoder', num_topics = 30)
model_all, _,_ = get_topics_model(all_108_upanishads.split('\n'), embedding_model = 'distiluse-base-multilingual-cased', num_topics = 30, speed = 'deep-learn')
result2d_all = get_embeddings_df_2d(model_all, num_reduced_topics = 10, sigma = [2.5,2.5])
result3d_all = get_embeddings_df_3d(model_all, num_reduced_topics = 10, sigma = [2.5, 2.5,2.5])
plot2d_df(result2d_all, 'bright', img_name = '108_upanishads_2d.pdf')
plot3d_df(result3d_all, 'bright', img_name = '108_upanishads_3d.pdf')
get_words(model_all)

# save topics
topic_words, word_scores, topic_nums = model_all.get_topics()
topic_dict = {}
topic_dict = {'multilingual-Bert-all-upanishads-topics':{'topic-words':topic_words.tolist(), 'word_score': word_scores.tolist()}}
with open('all_upanishads_topic.json', 'w') as fp:
  json.dump(topic_dict, fp, indent = 4)

